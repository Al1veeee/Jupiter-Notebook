{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Al1veeee/Jupiter-Notebook/blob/main/Alimkhanov_D_Workbook7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e126e007",
      "metadata": {
        "id": "e126e007"
      },
      "source": [
        "<h1>–ê–õ–ò–ú–•–ê–ù–û–í –î–ï–ù–ò</h1>\n",
        "<h2> –†–∞–±–æ—á–∞—è —Ç–µ—Ç—Ä–∞–¥—å</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ee1f12",
      "metadata": {
        "id": "c0ee1f12"
      },
      "source": [
        "–û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
        "–ü–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—É—é —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
        "–û–¥–∏–Ω–æ—á–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —è–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–º –±–∏–Ω–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º. –í\n",
        "—ç—Ç–æ–π –ª–µ–∫—Ü–∏–∏ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –¥–ª—è\n",
        "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ—Å–∫–æ–ª—å–∫—É –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π\n",
        "–±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Ç–æ –º—ã –±—É–¥–µ–º —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –ª–∏—à—å –¥–≤–∞ –∫–ª–∞—Å—Å–∞.\n",
        "–ü—É—Å—Ç—å –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ (–∫–æ–Ω–µ—á–Ω–æ–µ –∏–ª–∏\n",
        "–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ) n-–º–µ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ–º –æ–±–æ–∑–Ω–∞—á–∞—Ç—å ùë• =\n",
        "(ùë•1, ùë•2, . . . , ùë•ùëõ)\n",
        "–ë—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –¥–≤–∞ –∫–ª–∞—Å—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ\n",
        "–º—ã –±—É–¥–µ–º –æ–±–æ–∑–Ω–∞—á–∞—Ç—å +1 –∏ -1. –ü–æ—ç—Ç–æ–º—É –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∑–∞–¥–∞—á–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è\n",
        "—Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–¥–∞–Ω–∞ –Ω–∞ –Ω–∞—à–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤–µ–∫—Ç–æ—Ä–æ–≤, –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç\n",
        "–∑–Ω–∞—á–µ–Ω–∏—è –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ {+1, ‚àí1}. –í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–∂–µ—Ç –≤—ã—Å—Ç—É–ø–∞—Ç—å\n",
        "–ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω. –° –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞\n",
        "–≤–µ—Å–æ–≤ ùë§ = (ùë§0, ùë§1, ùë§2, . . . , ùë§ùëõ).</br>\n",
        "–ü—Ä–∏ —ç—Ç–æ–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —Ñ–æ—Ä–º—É–ª–µ</br>\n",
        "ùë¶ = ùë†ùëñùëîùëõ(ùë§0 + ùë•1ùë§1 + ùë•2ùë§2 + . . . + ùë•ùëõùë§ùëõ),</br>\n",
        "–≥–¥–µ —Ñ—É–Ω–∫—Ü–∏—è ùë†ùëñùëîùëõ(ùë°) —Ä–∞–≤–Ω–∞ +1, –µ—Å–ª–∏ ùë° ‚â• 0, –∏ —Ä–∞–≤–Ω–∞ ‚àí1, –µ—Å–ª–∏ ùë° < 0.\n",
        "–ü—Ä–∏–≤–µ–¥–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞. –ü—É—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä\n",
        "–æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö {(ùë•, ùëë)}, –≥–¥–µ ùë• - —ç—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞, –∞ ùëë –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞\n",
        "{+1, ‚àí1} —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –Ω–∞—à –≤–µ–∫—Ç–æ—Ä. </br>\n",
        "1. –ü–æ–ª–æ–∂–∏–º –≤–µ–∫—Ç–æ—Ä –≤–µ—Å–æ–≤ ùë§ —Ä–∞–≤–Ω—ã–º –Ω—É–ª—é.</br>\n",
        "2. –ü–æ–≤—Ç–æ—Ä—è—Ç—å ùëÅ —Ä–∞–∑ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:</br>\n",
        "3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (ùë•, ùëë):</br>\n",
        "4. –í—ã—á–∏—Å–ª–∏—Ç—å ùë¶ = ùë†ùëñùëîùëõ[(ùë•, ùë§)].</br>\n",
        "5. –ï—Å–ª–∏ ùë¶ùëë < 0, —Ç–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ ùë§0 = ùë§0 + ùëéùëë, ùë§ùëñ =\n",
        "ùë§ùëñ + ùëéùëëùë•ùëñ\n",
        ", ùëñ = 1,2, . . . , ùëõ.</br>\n",
        "–û–ø–∏—Å–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–æ–≤–æ–ª—å–Ω–æ –ª–µ–≥–∫–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaf7416",
      "metadata": {
        "id": "fdaf7416"
      },
      "source": [
        "<h1>1.1.1 –ü—Ä–∏–º–µ—Ä</h1>\n",
        "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ–≥—Ä–∞–º–º—É –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –Ω–∞ —è–∑—ã–∫–µ Python. –°–Ω–∞—á–∞–ª–∞\n",
        "—Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç —É—á–∏—Ç—å—Å—è –ø–æ\n",
        "—Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08268c1e",
      "metadata": {
        "id": "08268c1e",
        "outputId": "3566a6aa-b5dc-4666-fb6d-9434a6b5f448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1, -0.1]\n",
            "-1\n",
            "1\n",
            "1\n",
            "-1\n"
          ]
        }
      ],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä 1.1.1\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, N):\n",
        "        self.w = list()\n",
        "        for i in range(N):\n",
        "            self.w.append( 0)\n",
        "    def calc(self, x):\n",
        "        res = 0\n",
        "        for i in range(len(self.w)):\n",
        "            res = res + self.w[i] * x[i]\n",
        "        return res\n",
        "\n",
        "    def sign(self, x):\n",
        "        if self.calc(x) > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def learn(self, la, x, y):\n",
        "        if y * self.calc(x) <= 0:\n",
        "            for i in range(len(self.w)):\n",
        "                self.w[i] = self.w[i] + la * y * x[i]\n",
        "\n",
        "    def learning(self, la, T):\n",
        "        for n in range(100):\n",
        "            for t in T:\n",
        "                self.learn(la, t[ 0], t[1])\n",
        "\n",
        "perceptron = Perceptron(2)\n",
        "la = 0.1\n",
        "T = list()\n",
        "T.append([[2, 1], 1])\n",
        "T.append([[3, 2], 1])\n",
        "T.append([[4, 1], 1])\n",
        "T.append([[1, 2], -1])\n",
        "T.append([[2, 3], -1])\n",
        "T.append([[5, 7], -1])\n",
        "perceptron.learning(la, T)\n",
        "print(perceptron.w)\n",
        "print(perceptron.sign([1.5, 2]))\n",
        "print(perceptron.sign([3, 1.5]))\n",
        "print(perceptron.sign([5, 1]))\n",
        "print(perceptron.sign([5, 10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47612cc1",
      "metadata": {
        "id": "47612cc1"
      },
      "source": [
        "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∏–ª–∏\n",
        "–≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –∏–º–∏—Ç–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞,\n",
        "–ø–æ—Å–∫–æ–ª—å–∫—É –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. </br>\n",
        "–ù–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ç–∏–ø –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π\n",
        "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–æ–º (MLP), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è\n",
        "–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. MLP –∏–º–µ–µ—Ç –æ–¥–∏–Ω –≤—Ö–æ–¥–Ω–æ–π\n",
        "—Å–ª–æ–π –∏ –æ–¥–∏–Ω –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π. –ú–µ–∂–¥—É –Ω–∏–º–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ\n",
        "—Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤. –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –∏–º–µ–µ—Ç —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä –Ω–µ–π—Ä–æ–Ω–æ–≤, —á—Ç–æ –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏.\n",
        "–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞. –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω\n",
        "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ª–∏–Ω–µ–π–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è\n",
        "–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –í—ã—Ö–æ–¥ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –ø–æ–¥–∞–µ—Ç—Å—è –≤\n",
        "–∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª–æ–µ–≤.</br>\n",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å–ø–æ—Å–æ–±–Ω—ã —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á. –í –æ—Å–Ω–æ–≤–Ω–æ–º –æ–Ω–∏\n",
        "—Å–æ—Å—Ç–æ—è—Ç –∏–∑ —Ç–∞–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:</br>\n",
        "‚àí –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ø–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö);</br>\n",
        "‚àí —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π (–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ);</br>\n",
        "‚àí –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π. –ß—Ç–æ–±—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n",
        "–ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫ –≤–µ–¥—É—Ç —Å–µ–±—è –Ω–µ–π—Ä–æ–Ω—ã. –ù–µ–π—Ä–æ–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
        "–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–¥–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–¥–∞–µ—Ç\n",
        "–æ–¥–∏–Ω –≤—ã—Ö–æ–¥. –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–ª–æ–∫–∏ –≤–≤–æ–¥–∞ –∏\n",
        "–≤—ã–≤–æ–¥–∞, –≥–¥–µ –∫–∞–∂–¥–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞ (—ç—Ç–æ\n",
        "—Å–∏–ª–∞ —Å–≤—è–∑–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤; —á–µ–º –≤–µ—Å –±–æ–ª—å—à–µ, —Ç–µ–º –æ–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω —Å–∏–ª—å–Ω–µ–µ\n",
        "–≤–ª–∏—è–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–π). –î–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤ —É–º–Ω–æ–∂–∞—é—Ç—Å—è –Ω–∞ –≤–µ—Å–∞:</br>\n",
        "‚àí ùë• ‚Üí ùë• ‚àó ùë§1;</br>\n",
        "‚àí ùë¶ ‚Üí ùë¶ ‚àó ùë§2.</br>\n",
        "–í—Ö–æ–¥—ã –ø–æ—Å–ª–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Å—É–º–º–∏—Ä—É—é—Ç—Å—è —Å –ø—Ä–∏–±–∞–≤–ª–µ–Ω–∏–µ–º –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "–ø–æ—Ä–æ–≥–∞ ¬´c¬ª:</br>\n",
        "ùë•ùë§1 + ùë¶ùë§2 + ùëê</br>\n",
        "–ü–æ–ª—É—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "(—Å–∏–≥–º–æ–∏–¥—É), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥—ã –≤ –æ–¥–∏–Ω –≤—ã—Ö–æ–¥:</br>\n",
        "ùëß = ùëì(ùë•ùë§1 + ùë¶ùë§2 + ùëê).</br>\n",
        "–ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–≥–º–æ–∏–¥—ã ‚Äî –æ—Ç 0 –¥–æ 1. –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞\n",
        "—Å—Ç—Ä–µ–º—è—Ç—Å—è –∫ –Ω—É–ª—é, –∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ ‚Äî –∫ –µ–¥–∏–Ω–∏—Ü–µ.</br>\n",
        "–ù–∞–ø—Ä–∏–º–µ—Ä. –ü—É—Å—Ç—å –Ω–µ–π—Ä–æ–Ω –∏–º–µ–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: ùë§ = [0,1] ùëê = 4.\n",
        "–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: ùë• = 2, ùë¶ = 3.</br>\n",
        "((ùë•ùë§1) + (ùë¶ùë§2)) + ùëê = 20 + 31 + 4 = 7.</br>\n",
        "ùëß = ùëì(7) = 0.99.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b04ae23",
      "metadata": {
        "id": "1b04ae23"
      },
      "source": [
        "<h1>1.1.2 –ü—Ä–∏–º–µ—Ä</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f7c414",
      "metadata": {
        "id": "69f7c414",
        "outputId": "a2db6c34-ea01-42b6-ebf0-fbd901f441b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä 1.1.2\n",
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])\n",
        "bias = 4\n",
        "n = Neuron(weights, bias)\n",
        "x = np.array([2, 3])\n",
        "print(n.feedforward(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1f12a4",
      "metadata": {
        "id": "ad1f12a4",
        "outputId": "959f262a-c45a-49b7-d349-d145892b0f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7216325609518421\n"
          ]
        }
      ],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä\n",
        "import numpy as np\n",
        "class OurNeuralNetwork:\n",
        "    '''\n",
        "    –î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
        "        - –¥–≤–∞ –≤—Ö–æ–¥–∞\n",
        "        - –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (h1, h2)\n",
        "        - –≤—ã—Ö–æ–¥ (o1)\n",
        "    –ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
        "        - w = [0, 1]\n",
        "        - b = 0\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        weights = np.array([0, 1])\n",
        "        bias = 0\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "        return out_o1\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(network.feedforward(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83eca7c8",
      "metadata": {
        "id": "83eca7c8"
      },
      "source": [
        "–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Äî —ç—Ç–æ –ø–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤—Å–µ–º\n",
        "–≤—Ö–æ–¥–∞–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.\n",
        "–ö–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏:</br>\n",
        "–ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:</br>\n",
        "‚àí –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π –≤—ã—Ö–æ–¥);</br>\n",
        "‚àí –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π).</br>\n",
        "–ù–∞–ø—Ä–∏–º–µ—Ä:</br>\n",
        "–î–∞–Ω–∞ –¥–≤—É—Å–ª–æ–π–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å:</br>\n",
        "≈∑ = ùúé(ùë§2ùúé(ùë§1ùë• + ùëè1\n",
        ") + ùëè2\n",
        ").</br>\n",
        "–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞ –≤—ã—Ö–æ–¥ ≈∑ –≤–ª–∏—è—é—Ç —Ç–æ–ª—å–∫–æ –¥–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ‚Äî ùë§ (–≤–µ—Å–∞) –∏ ùëè\n",
        "(—Å–º–µ—â–µ–Ω–∏–µ). –ù–∞—Å—Ç—Ä–æ–π–∫—É –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–∞ –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å\n",
        "–æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º–æ–∂–Ω–æ –∏–∑–æ–±—Ä–∞–∑–∏—Ç—å —Ç–∞–∫:</br>\n",
        "–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ.</br>\n",
        "–ö–∞–∫ –≤–∏–¥–Ω–æ, —Ñ–æ—Ä–º—É–ª–∞ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π\n",
        "–Ω–µ—Å–ª–æ–∂–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ:</br>\n",
        "≈∑ = ùúé(ùë§2ùúé(ùë§1ùë• + ùëè1) + ùëè2)</br>\n",
        "–î–∞–ª–µ–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è.\n",
        "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Å–º–µ—â–µ–Ω–∏—è –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±—É–¥—É—Ç —Ä–∞–≤–Ω—ã 0.</br>\n",
        "–ß—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –æ—à–∏–±–∫—É –ø—Ä–æ–≥–Ω–æ–∑–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é\n",
        "–ø–æ—Ç–µ—Ä–∏. </br>\n",
        "–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ</br>\n",
        "–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–º–µ—Ä–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤\n",
        "–æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ ‚Äî –æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É, –∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è.\n",
        "–î–ª—è —ç—Ç–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∑–Ω–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä–∏ ‚Äî —Ç–∞–Ω–≥–µ–Ω—Å —É–≥–ª–∞\n",
        "–Ω–∞–∫–ª–æ–Ω–∞.</br>\n",
        "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –≤–µ—Å–∞–º –∏ —Å–º–µ—â–µ–Ω–∏—è–º –ø–æ–∑–≤–æ–ª—è–µ—Ç\n",
        "—É–∑–Ω–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫</br>\n",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –±–∞–∑–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –∏\n",
        "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö. –°–Ω–∞—á–∞–ª–∞ –º–æ–∂–µ—Ç –∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ –Ω–∏—Ö\n",
        "–¥–æ–≤–æ–ª—å–Ω–æ —Å–ª–æ–∂–Ω–æ. –ù–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≥–æ—Ç–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "–¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –Ω–µ —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ –∏—Ö\n",
        "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.</br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed8c438",
      "metadata": {
        "id": "2ed8c438",
        "outputId": "beec3466-e371-4eba-9f12-a2c23045559e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8170206500405062\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ\n",
        "# –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork\n",
        "# –ü–µ—Ä–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å\n",
        "'''\n",
        " –î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
        "        - —Ç—Ä–∏ –≤—Ö–æ–¥–∞\n",
        "        - —Ç—Ä–∏ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö\n",
        "        - –≤—ã—Ö–æ–¥ (o1)\n",
        " –ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
        "        - w = [0.5, 0.5, 0.5]\n",
        "        - b = 0\n",
        "'''\n",
        "import numpy as np\n",
        "class OurNeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        weights = np.array([0.5, 0.5, 0.5])\n",
        "        bias = 0\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        self.h3 = Neuron(weights, bias)\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        out_h3 = self.h3.feedforward(x)\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
        "        return out_o1\n",
        "\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([3, 4, 5])\n",
        "print(network.feedforward(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4056b576",
      "metadata": {
        "id": "4056b576"
      },
      "source": [
        "<h1>–ó–∞–¥–∞–Ω–∏–µ</h1>\n",
        "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork.\n",
        "–î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:</br>\n",
        "‚àí —Ç—Ä–∏ –≤—Ö–æ–¥–∞ (ùë•1, ùë•2, ùë•3\n",
        ");</br>\n",
        "‚àí —Ç—Ä–∏ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1, ‚Ñé2, ‚Ñé3);</br>\n",
        "‚àí –≤—ã—Ö–æ–¥ (ùëú1).</br>\n",
        "–ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:</br>\n",
        "‚àí ùë§ = [0.5, 0.5, 0.5]</br>\n",
        "‚àí ùëè = 0</br>\n",
        "–î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:</br>\n",
        "‚àí –¥–≤–∞ –≤—Ö–æ–¥–∞ (ùë•1, ùë•2);</br>\n",
        "‚àí –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1, ‚Ñé2);</br>\n",
        "‚àí –¥–≤–∞ –≤—ã—Ö–æ–¥–∞ (ùëú1, ùëú2\n",
        ").</br>\n",
        "–ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:</br>\n",
        "‚àí ùë§ = [1, 0];</br>\n",
        "‚àí ùëè = 1</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2e52de",
      "metadata": {
        "id": "6f2e52de",
        "outputId": "a2088b58-a454-499f-905e-c33adbbac975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8788956696366197\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ\n",
        "# –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork\n",
        "# –í—Ç–æ—Ä–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å\n",
        "'''\n",
        " –î–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
        "        - –¥–≤–∞ –≤—Ö–æ–¥–∞\n",
        "        - –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö\n",
        "        - –¥–≤–∞ –≤—ã—Ö–æ–¥–∞\n",
        " –ù–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
        "        - w = [1, 0]\n",
        "        - b = 1\n",
        "'''\n",
        "import numpy as np\n",
        "class OurNeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        weights = np.array([1, 0])\n",
        "        bias = 1\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "        self.o2 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "        return out_o1\n",
        "\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([3, 4])\n",
        "print(network.feedforward(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd178a09",
      "metadata": {
        "id": "dd178a09"
      },
      "source": [
        "<h1>–ó–∞–¥–∞–Ω–∏–µ</h1>\n",
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π\n",
        "–∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f10f504",
      "metadata": {
        "id": "4f10f504",
        "outputId": "daab413c-e150-4ac1-cc0e-f2152af3e84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9820137900379085\n",
            "1.1578212823495777\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ\n",
        "# –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
        "import numpy as np\n",
        "from math import tan\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return tan(x)\n",
        "\n",
        "def ReLU(x):\n",
        "    return max(0, x)\n",
        "\n",
        "class Neuron_sigmoid:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "class Neuron_tanh:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return tanh(total)\n",
        "\n",
        "class Neuron_ReLU:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return max(0, total)\n",
        "\n",
        "weights = np.array([0, 1])\n",
        "bias = 1\n",
        "n1 = Neuron_sigmoid(weights, bias)\n",
        "n2 = Neuron_tanh(weights, bias)\n",
        "n3 = Neuron_ReLU(weights, bias)\n",
        "x = np.array([2, 3])\n",
        "print(n1.feedforward(x))\n",
        "print(n2.feedforward(x))\n",
        "print(n3.feedforward(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedb2d93",
      "metadata": {
        "id": "cedb2d93"
      },
      "source": [
        "–¢–µ–ø–µ—Ä—å –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –∫–∞–∫–∏–µ —à–∞–≥–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n",
        "–≤—ã–ø–æ–ª–Ω–∏—Ç—å, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å —Å –ø–ª–æ—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏.\n",
        "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è\n",
        "–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –¥–∞–Ω–Ω–æ–µ —Ä–∞—Å—Ç–µ–Ω–∏–µ –∏—Ä–∏—Å–∞. –ú—ã\n",
        "–±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É Python Scikit-Learn –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—à–µ–π\n",
        "–Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.</br>\n",
        "Sklearn –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç 2 –æ—Ü–µ–Ω—â–∏–∫–∞ –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏\n",
        "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ:</br>\n",
        "‚àí MLPClassifier;</br>\n",
        "‚àí MLPRegressor</br>\n",
        "–ù–∞—á–Ω–µ–º —Å –∏–º–ø–æ—Ä—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.</br>\n",
        "\n",
        "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö</br>\n",
        "–ú—ã –±—É–¥–µ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</br>\n",
        "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ñ—Ä: –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ü–∏—Ñ—Ä,\n",
        "–∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º 8x8 –¥–ª—è —Ü–∏—Ñ—Ä 0-9. –ù–∏–∂–µ –º—ã –±—É–¥–µ–º\n",
        "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.</br>\n",
        "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ: –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä\n",
        "–¥–∞–Ω–Ω—ã—Ö –æ –∂–∏–ª—å–µ –≤ –ë–æ—Å—Ç–æ–Ω–µ, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö\n",
        "—Å–≤–æ–π—Å—Ç–≤–∞—Ö –¥–æ–º–∞, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç, —É—Ä–æ–≤–µ–Ω—å –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏\n",
        "–Ω–∞ –¥—É—à—É –Ω–∞—Å–µ–ª–µ–Ω–∏—è –≤ –≥–æ—Ä–æ–¥–µ –∏ —Ç. –¥. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –∑–∞–¥–∞—á\n",
        "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏.</br>\n",
        "Sklearn –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∞ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å\n",
        "–∏—Ö, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥—ã load_digits() –∏ load_boston().</br>\n",
        "\n",
        "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</br>\n",
        "MLPClassifier ‚Äî —ç—Ç–æ –∫–ª–≤—Å—Å, –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–∞–∫ —á–∞—Å—Ç—å –º–æ–¥—É–ª—è neuro_network\n",
        "sklearn –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º\n",
        "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞.</br>\n",
        "–ö–∞–∫ –æ–±—ã—á–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏:</br>\n",
        "‚àí –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "–æ–±—É—á–µ–Ω–∏—è;</br>\n",
        "‚àí —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å\n",
        "–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.</br>\n",
        "–§—É–Ω–∫—Ü–∏—è train_test_split –º–æ–¥—É–ª—è model_selection sklearn –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º\n",
        "—Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: 80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ 20% –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
        "–ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º seed(random_state=123) —Å train_test_split, —á—Ç–æ–±—ã –º—ã\n",
        "–≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–ª–∏ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –º–æ–≥–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏\n",
        "–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±—É–¥—É—â–µ–º.</br>\n",
        "–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å MLPClassifier —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "–¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</br>\n",
        "C–æ–∑–¥–∞–¥–∏–º –º–µ—Ç–æ–¥ plot_confusion_matrix(), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –∏\n",
        "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –æ–Ω —Å—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã,\n",
        "–∏—Å–ø–æ–ª—å–∑—É—è matplotlib.</br>\n",
        "–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω —Å–ø–∏—Å–æ–∫ –≤–∞–∂–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å MLPClassifier,\n",
        "–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–Ω–∞—á–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "–º–æ–¥–µ–ª–∏.</br>\n",
        "‚àí loss_ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–±—ã—Ç–æ–∫ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.</br>\n",
        "‚àí coefs_ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –¥–ª–∏–Ω—ã n_layers-1, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
        "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Å–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —É—Ä–æ–≤–Ω–µ–º i.</br>\n",
        "‚àí intercepts_ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –¥–ª–∏–Ω—ã n_layers-1, –≥–¥–µ –∫–∞–∂–¥—ã–π\n",
        "—ç–ª–µ–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–µ—Ö–≤–∞—Ç, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞–º–∏\n",
        "—Å–ª–æ—è i.</br>\n",
        "‚àí n_iter_ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã–ø–æ–ª–Ω—è–ª–∞—Å—å –æ—Ü–µ–Ω–∫–∞.</br>\n",
        "‚àí out_activation_ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ\n",
        "—Å–ª–æ—è.</br>\n",
        "MLPRegressor ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å, –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–∞–∫ —á–∞—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "neuro_network sklearn –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º\n",
        "–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞. –¢–∞–∫–∂–µ —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏:</br>\n",
        "‚àí –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è (80%), –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è\n",
        "–º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è;</br>\n",
        "‚àí —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (20%), –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å\n",
        "–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a349e95",
      "metadata": {
        "id": "4a349e95",
        "outputId": "b02f90ac-70e6-4313-81ed-06c5f38dff3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Sizes :  (1797, 64) (1797,)\n",
            "Dataset Sizes :  (506, 13) (506,)\n",
            "Train/Test Sizes :  (1437, 64) (360, 64) (1437,) (360,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ivanu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5 9 9 6 1 6 6 9 8 7 4 2 1 4 3]\n",
            "[5 9 9 6 1 6 6 9 8 7 4 2 1 4 3]\n",
            "Test Accuracy: 0.983\n",
            "Training Accurace : 1.000\n",
            "Loss :  0.0034728684994180608\n",
            "Number of Coefs :  2\n",
            "Number of Intercepts :  2\n",
            "Number of Interception for Which Estimator Ran :  125\n",
            "Name of Output Layer Activation Function :  softmax\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits, load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "digits = load_digits()\n",
        "X_digits, Y_digits = digits.data, digits.target\n",
        "print('Dataset Sizes : ', X_digits.shape, Y_digits.shape)\n",
        "boston = load_boston()\n",
        "X_boston, Y_boston = boston.data, boston.target\n",
        "print('Dataset Sizes : ', X_boston.shape, Y_boston.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size = 0.80, test_size = 0.20, stratify = Y_digits, random_state = 123)\n",
        "print('Train/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "\n",
        "mlp_classifier = MLPClassifier(random_state = 123)\n",
        "mlp_classifier.fit(X_train, Y_train)\n",
        "\n",
        "Y_preds = mlp_classifier.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "print('Test Accuracy: %.3f'%mlp_classifier.score(X_test, Y_test))\n",
        "\n",
        "print('Training Accurace : %.3f'%mlp_classifier.score(X_train, Y_train))\n",
        "\n",
        "def plot_confusion_matrix(Y_test, Y_preds):\n",
        "    conf_mat = confusion_matrix(Y_test, Y_preds)\n",
        "    fig = plt.figure(figsize = (6, 6))\n",
        "    plt.matshow(conf_mat, cmap = plt.cm.Blues, fignum = 1)\n",
        "    plt.yticks(range(10), range(10))\n",
        "    plt.xticks(range(10), range(10))\n",
        "    plt.colorbar();\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            plt.text(i - 0.2, j + 0.1, str(conf_mat[j, i]), color = 'tab:red')\n",
        "plot_confusion_matrix(Y_test, mlp_classifier.predict(X_test))\n",
        "\n",
        "print(\"Loss : \", mlp_classifier.loss_)\n",
        "print(\"Number of Coefs : \", len(mlp_classifier.coefs_))\n",
        "print(\"Number of Intercepts : \", len(mlp_classifier.intercepts_))\n",
        "print(\"Number of Interception for Which Estimator Ran : \", mlp_classifier.n_iter_)\n",
        "print(\"Name of Output Layer Activation Function : \", mlp_classifier.out_activation_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529e3424",
      "metadata": {
        "id": "529e3424"
      },
      "source": [
        "<h1>–ó–∞–¥–∞–Ω–∏–µ</h1>\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã MLPClassified –∏ MLPRegressor –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏\n",
        "—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑\n",
        "–∞—Ç—Ä–∏–±—É—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
        "–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–µ—Ç–µ –≤–∑—è—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ò—Ä–∏—Å–æ–≤:\n",
        "https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f\n",
        "7d537619d0e07d5ae3/iris.csv\n",
        "–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã:\n",
        "https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linearregression/master/Salary_Data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a12bf79",
      "metadata": {
        "id": "4a12bf79",
        "outputId": "a49108a0-2d11-470c-b9ab-56655fde1403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Sizes :  (150, 4) (150,)\n",
            "Train/Test Sizes :  (120, 4) (30, 4) (120,) (30,)\n",
            "Iteration 1, loss = 1.13315020\n",
            "Iteration 2, loss = 1.11956081\n",
            "Iteration 3, loss = 1.10066325\n",
            "Iteration 4, loss = 1.07779384\n",
            "Iteration 5, loss = 1.05331231\n",
            "Iteration 6, loss = 1.02898957\n",
            "Iteration 7, loss = 1.00592943\n",
            "Iteration 8, loss = 0.98421978\n",
            "Iteration 9, loss = 0.96405607\n",
            "Iteration 10, loss = 0.94609246\n",
            "Iteration 11, loss = 0.93091433\n",
            "Iteration 12, loss = 0.91871454\n",
            "Iteration 13, loss = 0.90905920\n",
            "Iteration 14, loss = 0.90150382\n",
            "Iteration 15, loss = 0.89452336\n",
            "Iteration 16, loss = 0.88746732\n",
            "Iteration 17, loss = 0.88005350\n",
            "Iteration 18, loss = 0.87211449\n",
            "Iteration 19, loss = 0.86375679\n",
            "Iteration 20, loss = 0.85510491\n",
            "Iteration 21, loss = 0.84628451\n",
            "Iteration 22, loss = 0.83742688\n",
            "Iteration 23, loss = 0.82858149\n",
            "Iteration 24, loss = 0.81983207\n",
            "Iteration 25, loss = 0.81121928\n",
            "Iteration 26, loss = 0.80279998\n",
            "Iteration 27, loss = 0.79454477\n",
            "Iteration 28, loss = 0.78627793\n",
            "Iteration 29, loss = 0.77804891\n",
            "Iteration 30, loss = 0.76994925\n",
            "Iteration 31, loss = 0.76195424\n",
            "Iteration 32, loss = 0.75398967\n",
            "Iteration 33, loss = 0.74599281\n",
            "Iteration 34, loss = 0.73799865\n",
            "Iteration 35, loss = 0.73023199\n",
            "Iteration 36, loss = 0.72260848\n",
            "Iteration 37, loss = 0.71511062\n",
            "Iteration 38, loss = 0.70786123\n",
            "Iteration 39, loss = 0.70083317\n",
            "Iteration 40, loss = 0.69401390\n",
            "Iteration 41, loss = 0.68736245\n",
            "Iteration 42, loss = 0.68083168\n",
            "Iteration 43, loss = 0.67442992\n",
            "Iteration 44, loss = 0.66811997\n",
            "Iteration 45, loss = 0.66184438\n",
            "Iteration 46, loss = 0.65559608\n",
            "Iteration 47, loss = 0.64943646\n",
            "Iteration 48, loss = 0.64345026\n",
            "Iteration 49, loss = 0.63764258\n",
            "Iteration 50, loss = 0.63197777\n",
            "Iteration 51, loss = 0.62641066\n",
            "Iteration 52, loss = 0.62087836\n",
            "Iteration 53, loss = 0.61536846\n",
            "Iteration 54, loss = 0.60988038\n",
            "Iteration 55, loss = 0.60440101\n",
            "Iteration 56, loss = 0.59890366\n",
            "Iteration 57, loss = 0.59342448\n",
            "Iteration 58, loss = 0.58801147\n",
            "Iteration 59, loss = 0.58263051\n",
            "Iteration 60, loss = 0.57732853\n",
            "Iteration 61, loss = 0.57216829\n",
            "Iteration 62, loss = 0.56723262\n",
            "Iteration 63, loss = 0.56248811\n",
            "Iteration 64, loss = 0.55789648\n",
            "Iteration 65, loss = 0.55345734\n",
            "Iteration 66, loss = 0.54910997\n",
            "Iteration 67, loss = 0.54479056\n",
            "Iteration 68, loss = 0.54051121\n",
            "Iteration 69, loss = 0.53629030\n",
            "Iteration 70, loss = 0.53212791\n",
            "Iteration 71, loss = 0.52801525\n",
            "Iteration 72, loss = 0.52395588\n",
            "Iteration 73, loss = 0.51992967\n",
            "Iteration 74, loss = 0.51593112\n",
            "Iteration 75, loss = 0.51196029\n",
            "Iteration 76, loss = 0.50802205\n",
            "Iteration 77, loss = 0.50410889\n",
            "Iteration 78, loss = 0.50020755\n",
            "Iteration 79, loss = 0.49624562\n",
            "Iteration 80, loss = 0.49222730\n",
            "Iteration 81, loss = 0.48815985\n",
            "Iteration 82, loss = 0.48401052\n",
            "Iteration 83, loss = 0.47981993\n",
            "Iteration 84, loss = 0.47546136\n",
            "Iteration 85, loss = 0.47107200\n",
            "Iteration 86, loss = 0.46686159\n",
            "Iteration 87, loss = 0.46289461\n",
            "Iteration 88, loss = 0.45916698\n",
            "Iteration 89, loss = 0.45562100\n",
            "Iteration 90, loss = 0.45219033\n",
            "Iteration 91, loss = 0.44891735\n",
            "Iteration 92, loss = 0.44574607\n",
            "Iteration 93, loss = 0.44267020\n",
            "Iteration 94, loss = 0.43970881\n",
            "Iteration 95, loss = 0.43683444\n",
            "Iteration 96, loss = 0.43401800\n",
            "Iteration 97, loss = 0.43127902\n",
            "Iteration 98, loss = 0.42857318\n",
            "Iteration 99, loss = 0.42590028\n",
            "Iteration 100, loss = 0.42326067\n",
            "Iteration 101, loss = 0.42065051\n",
            "Iteration 102, loss = 0.41807177\n",
            "Iteration 103, loss = 0.41552555\n",
            "Iteration 104, loss = 0.41301335\n",
            "Iteration 105, loss = 0.41053252\n",
            "Iteration 106, loss = 0.40808330\n",
            "Iteration 107, loss = 0.40566078\n",
            "Iteration 108, loss = 0.40327079\n",
            "Iteration 109, loss = 0.40091192\n",
            "Iteration 110, loss = 0.39858107\n",
            "Iteration 111, loss = 0.39628014\n",
            "Iteration 112, loss = 0.39401063\n",
            "Iteration 113, loss = 0.39176829\n",
            "Iteration 114, loss = 0.38955477\n",
            "Iteration 115, loss = 0.38736645\n",
            "Iteration 116, loss = 0.38520604\n",
            "Iteration 117, loss = 0.38307115\n",
            "Iteration 118, loss = 0.38095675\n",
            "Iteration 119, loss = 0.37886287\n",
            "Iteration 120, loss = 0.37679137\n",
            "Iteration 121, loss = 0.37474182\n",
            "Iteration 122, loss = 0.37271187\n",
            "Iteration 123, loss = 0.37070545\n",
            "Iteration 124, loss = 0.36871961\n",
            "Iteration 125, loss = 0.36675336\n",
            "Iteration 126, loss = 0.36480943\n",
            "Iteration 127, loss = 0.36288335\n",
            "Iteration 128, loss = 0.36097221\n",
            "Iteration 129, loss = 0.35907608\n",
            "Iteration 130, loss = 0.35720127\n",
            "Iteration 131, loss = 0.35535058\n",
            "Iteration 132, loss = 0.35351935\n",
            "Iteration 133, loss = 0.35170497\n",
            "Iteration 134, loss = 0.34990674\n",
            "Iteration 135, loss = 0.34812638\n",
            "Iteration 136, loss = 0.34636069\n",
            "Iteration 137, loss = 0.34460923\n",
            "Iteration 138, loss = 0.34287174\n",
            "Iteration 139, loss = 0.34114865\n",
            "Iteration 140, loss = 0.33944046\n",
            "Iteration 141, loss = 0.33774613\n",
            "Iteration 142, loss = 0.33606557\n",
            "Iteration 143, loss = 0.33439828\n",
            "Iteration 144, loss = 0.33274529\n",
            "Iteration 145, loss = 0.33110597\n",
            "Iteration 146, loss = 0.32947926\n",
            "Iteration 147, loss = 0.32786464\n",
            "Iteration 148, loss = 0.32626148\n",
            "Iteration 149, loss = 0.32467024\n",
            "Iteration 150, loss = 0.32309081\n",
            "Iteration 151, loss = 0.32152244\n",
            "Iteration 152, loss = 0.31996420\n",
            "Iteration 153, loss = 0.31841649\n",
            "Iteration 154, loss = 0.31687864\n",
            "Iteration 155, loss = 0.31535068\n",
            "Iteration 156, loss = 0.31383232\n",
            "Iteration 157, loss = 0.31232454\n",
            "Iteration 158, loss = 0.31082660\n",
            "Iteration 159, loss = 0.30933809\n",
            "Iteration 160, loss = 0.30786002\n",
            "Iteration 161, loss = 0.30639150\n",
            "Iteration 162, loss = 0.30493314\n",
            "Iteration 163, loss = 0.30348378\n",
            "Iteration 164, loss = 0.30204383\n",
            "Iteration 165, loss = 0.30061270\n",
            "Iteration 166, loss = 0.29919087\n",
            "Iteration 167, loss = 0.29777777\n",
            "Iteration 168, loss = 0.29637357\n",
            "Iteration 169, loss = 0.29497899\n",
            "Iteration 170, loss = 0.29359285\n",
            "Iteration 171, loss = 0.29221472\n",
            "Iteration 172, loss = 0.29084477\n",
            "Iteration 173, loss = 0.28948305\n",
            "Iteration 174, loss = 0.28813056\n",
            "Iteration 175, loss = 0.28678790\n",
            "Iteration 176, loss = 0.28545329\n",
            "Iteration 177, loss = 0.28412682\n",
            "Iteration 178, loss = 0.28280856\n",
            "Iteration 179, loss = 0.28149777\n",
            "Iteration 180, loss = 0.28019443\n",
            "Iteration 181, loss = 0.27889823\n",
            "Iteration 182, loss = 0.27760952\n",
            "Iteration 183, loss = 0.27632816\n",
            "Iteration 184, loss = 0.27505372\n",
            "Iteration 185, loss = 0.27378626\n",
            "Iteration 186, loss = 0.27252630\n",
            "Iteration 187, loss = 0.27127320\n",
            "Iteration 188, loss = 0.27002718\n",
            "Iteration 189, loss = 0.26878792\n",
            "Iteration 190, loss = 0.26755560\n",
            "Iteration 191, loss = 0.26633031\n",
            "Iteration 192, loss = 0.26511234\n",
            "Iteration 193, loss = 0.26390121\n",
            "Iteration 194, loss = 0.26269673\n",
            "Iteration 195, loss = 0.26149914\n",
            "Iteration 196, loss = 0.26030827\n",
            "Iteration 197, loss = 0.25912446\n",
            "Iteration 198, loss = 0.25794779\n",
            "Iteration 199, loss = 0.25677735\n",
            "Iteration 200, loss = 0.25561319\n",
            "Iteration 201, loss = 0.25445536\n",
            "Iteration 202, loss = 0.25330394\n",
            "Iteration 203, loss = 0.25215894\n",
            "Iteration 204, loss = 0.25102047\n",
            "Iteration 205, loss = 0.24988850\n",
            "Iteration 206, loss = 0.24876267\n",
            "Iteration 207, loss = 0.24764345\n",
            "Iteration 208, loss = 0.24653125\n",
            "Iteration 209, loss = 0.24542521\n",
            "Iteration 210, loss = 0.24432553\n",
            "Iteration 211, loss = 0.24323229\n",
            "Iteration 212, loss = 0.24214507\n",
            "Iteration 213, loss = 0.24106388\n",
            "Iteration 214, loss = 0.23998865\n",
            "Iteration 215, loss = 0.23892005\n",
            "Iteration 216, loss = 0.23785781\n",
            "Iteration 217, loss = 0.23680190\n",
            "Iteration 218, loss = 0.23575203\n",
            "Iteration 219, loss = 0.23470826\n",
            "Iteration 220, loss = 0.23367031\n",
            "Iteration 221, loss = 0.23263824\n",
            "Iteration 222, loss = 0.23161209\n",
            "Iteration 223, loss = 0.23059197\n",
            "Iteration 224, loss = 0.22957811\n",
            "Iteration 225, loss = 0.22857038\n",
            "Iteration 226, loss = 0.22756882\n",
            "Iteration 227, loss = 0.22657295\n",
            "Iteration 228, loss = 0.22558248\n",
            "Iteration 229, loss = 0.22459762\n",
            "Iteration 230, loss = 0.22361851\n",
            "Iteration 231, loss = 0.22264544\n",
            "Iteration 232, loss = 0.22167813\n",
            "Iteration 233, loss = 0.22071647\n",
            "Iteration 234, loss = 0.21976075\n",
            "Iteration 235, loss = 0.21881110\n",
            "Iteration 236, loss = 0.21786727\n",
            "Iteration 237, loss = 0.21692930\n",
            "Iteration 238, loss = 0.21599716\n",
            "Iteration 239, loss = 0.21507063\n",
            "Iteration 240, loss = 0.21414962\n",
            "Iteration 241, loss = 0.21323399\n",
            "Iteration 242, loss = 0.21232381\n",
            "Iteration 243, loss = 0.21141902\n",
            "Iteration 244, loss = 0.21051973\n",
            "Iteration 245, loss = 0.20962601\n",
            "Iteration 246, loss = 0.20873767\n",
            "Iteration 247, loss = 0.20785482\n",
            "Iteration 248, loss = 0.20697737\n",
            "Iteration 249, loss = 0.20610531\n",
            "Iteration 250, loss = 0.20523870\n",
            "Iteration 251, loss = 0.20437734\n",
            "Iteration 252, loss = 0.20352133\n",
            "Iteration 253, loss = 0.20267064\n",
            "Iteration 254, loss = 0.20182531\n",
            "Iteration 255, loss = 0.20098514\n",
            "Iteration 256, loss = 0.20015024\n",
            "Iteration 257, loss = 0.19932052\n",
            "Iteration 258, loss = 0.19849600\n",
            "Iteration 259, loss = 0.19767670\n",
            "Iteration 260, loss = 0.19686257\n",
            "Iteration 261, loss = 0.19605360\n",
            "Iteration 262, loss = 0.19524975\n",
            "Iteration 263, loss = 0.19445101\n",
            "Iteration 264, loss = 0.19365731\n",
            "Iteration 265, loss = 0.19286862\n",
            "Iteration 266, loss = 0.19208483\n",
            "Iteration 267, loss = 0.19130592\n",
            "Iteration 268, loss = 0.19053218\n",
            "Iteration 269, loss = 0.18976327\n",
            "Iteration 270, loss = 0.18899899\n",
            "Iteration 271, loss = 0.18823953\n",
            "Iteration 272, loss = 0.18748487\n",
            "Iteration 273, loss = 0.18673507\n",
            "Iteration 274, loss = 0.18599006\n",
            "Iteration 275, loss = 0.18524993\n",
            "Iteration 276, loss = 0.18451449\n",
            "Iteration 277, loss = 0.18378384\n",
            "Iteration 278, loss = 0.18305806\n",
            "Iteration 279, loss = 0.18233699\n",
            "Iteration 280, loss = 0.18162057\n",
            "Iteration 281, loss = 0.18090863\n",
            "Iteration 282, loss = 0.18020107\n",
            "Iteration 283, loss = 0.17949806\n",
            "Iteration 284, loss = 0.17879957\n",
            "Iteration 285, loss = 0.17810552\n",
            "Iteration 286, loss = 0.17741584\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 287, loss = 0.17673066\n",
            "Iteration 288, loss = 0.17604993\n",
            "Iteration 289, loss = 0.17537339\n",
            "Iteration 290, loss = 0.17470143\n",
            "Iteration 291, loss = 0.17403374\n",
            "Iteration 292, loss = 0.17337016\n",
            "Iteration 293, loss = 0.17271089\n",
            "Iteration 294, loss = 0.17205581\n",
            "Iteration 295, loss = 0.17140484\n",
            "Iteration 296, loss = 0.17075797\n",
            "Iteration 297, loss = 0.17011516\n",
            "Iteration 298, loss = 0.16947651\n",
            "Iteration 299, loss = 0.16884191\n",
            "Iteration 300, loss = 0.16821132\n",
            "Iteration 301, loss = 0.16758471\n",
            "Iteration 302, loss = 0.16696195\n",
            "Iteration 303, loss = 0.16634306\n",
            "Iteration 304, loss = 0.16572794\n",
            "Iteration 305, loss = 0.16511703\n",
            "Iteration 306, loss = 0.16451025\n",
            "Iteration 307, loss = 0.16390736\n",
            "Iteration 308, loss = 0.16330843\n",
            "Iteration 309, loss = 0.16271340\n",
            "Iteration 310, loss = 0.16212205\n",
            "Iteration 311, loss = 0.16153446\n",
            "Iteration 312, loss = 0.16095042\n",
            "Iteration 313, loss = 0.16037017\n",
            "Iteration 314, loss = 0.15979370\n",
            "Iteration 315, loss = 0.15922105\n",
            "Iteration 316, loss = 0.15865223\n",
            "Iteration 317, loss = 0.15808722\n",
            "Iteration 318, loss = 0.15752586\n",
            "Iteration 319, loss = 0.15696805\n",
            "Iteration 320, loss = 0.15641374\n",
            "Iteration 321, loss = 0.15586306\n",
            "Iteration 322, loss = 0.15531589\n",
            "Iteration 323, loss = 0.15477235\n",
            "Iteration 324, loss = 0.15423226\n",
            "Iteration 325, loss = 0.15369568\n",
            "Iteration 326, loss = 0.15316259\n",
            "Iteration 327, loss = 0.15263295\n",
            "Iteration 328, loss = 0.15210669\n",
            "Iteration 329, loss = 0.15158358\n",
            "Iteration 330, loss = 0.15106375\n",
            "Iteration 331, loss = 0.15054698\n",
            "Iteration 332, loss = 0.15003311\n",
            "Iteration 333, loss = 0.14952256\n",
            "Iteration 334, loss = 0.14901522\n",
            "Iteration 335, loss = 0.14851098\n",
            "Iteration 336, loss = 0.14800999\n",
            "Iteration 337, loss = 0.14751261\n",
            "Iteration 338, loss = 0.14701851\n",
            "Iteration 339, loss = 0.14652761\n",
            "Iteration 340, loss = 0.14603968\n",
            "Iteration 341, loss = 0.14555457\n",
            "Iteration 342, loss = 0.14507240\n",
            "Iteration 343, loss = 0.14459331\n",
            "Iteration 344, loss = 0.14411716\n",
            "Iteration 345, loss = 0.14364398\n",
            "Iteration 346, loss = 0.14317450\n",
            "Iteration 347, loss = 0.14270839\n",
            "Iteration 348, loss = 0.14224536\n",
            "Iteration 349, loss = 0.14178539\n",
            "Iteration 350, loss = 0.14132864\n",
            "Iteration 351, loss = 0.14087438\n",
            "Iteration 352, loss = 0.14042249\n",
            "Iteration 353, loss = 0.13997340\n",
            "Iteration 354, loss = 0.13952714\n",
            "Iteration 355, loss = 0.13908362\n",
            "Iteration 356, loss = 0.13864288\n",
            "Iteration 357, loss = 0.13820489\n",
            "Iteration 358, loss = 0.13776995\n",
            "Iteration 359, loss = 0.13733771\n",
            "Iteration 360, loss = 0.13690802\n",
            "Iteration 361, loss = 0.13648088\n",
            "Iteration 362, loss = 0.13605624\n",
            "Iteration 363, loss = 0.13563411\n",
            "Iteration 364, loss = 0.13521461\n",
            "Iteration 365, loss = 0.13479778\n",
            "Iteration 366, loss = 0.13438356\n",
            "Iteration 367, loss = 0.13397216\n",
            "Iteration 368, loss = 0.13356337\n",
            "Iteration 369, loss = 0.13315711\n",
            "Iteration 370, loss = 0.13275334\n",
            "Iteration 371, loss = 0.13235213\n",
            "Iteration 372, loss = 0.13195363\n",
            "Iteration 373, loss = 0.13155768\n",
            "Iteration 374, loss = 0.13116448\n",
            "Iteration 375, loss = 0.13077373\n",
            "Iteration 376, loss = 0.13038547\n",
            "Iteration 377, loss = 0.12999962\n",
            "Iteration 378, loss = 0.12961620\n",
            "Iteration 379, loss = 0.12923497\n",
            "Iteration 380, loss = 0.12885606\n",
            "Iteration 381, loss = 0.12847950\n",
            "Iteration 382, loss = 0.12810509\n",
            "Iteration 383, loss = 0.12773301\n",
            "Iteration 384, loss = 0.12736312\n",
            "Iteration 385, loss = 0.12699536\n",
            "Iteration 386, loss = 0.12662968\n",
            "Iteration 387, loss = 0.12626601\n",
            "Iteration 388, loss = 0.12590470\n",
            "Iteration 389, loss = 0.12554544\n",
            "Iteration 390, loss = 0.12518814\n",
            "Iteration 391, loss = 0.12483301\n",
            "Iteration 392, loss = 0.12448002\n",
            "Iteration 393, loss = 0.12412917\n",
            "Iteration 394, loss = 0.12378041\n",
            "Iteration 395, loss = 0.12343334\n",
            "Iteration 396, loss = 0.12308800\n",
            "Iteration 397, loss = 0.12274470\n",
            "Iteration 398, loss = 0.12240361\n",
            "Iteration 399, loss = 0.12206473\n",
            "Iteration 400, loss = 0.12172798\n",
            "Iteration 401, loss = 0.12139353\n",
            "Iteration 402, loss = 0.12106114\n",
            "Iteration 403, loss = 0.12073086\n",
            "Iteration 404, loss = 0.12040282\n",
            "Iteration 405, loss = 0.12007704\n",
            "Iteration 406, loss = 0.11975327\n",
            "Iteration 407, loss = 0.11943159\n",
            "Iteration 408, loss = 0.11911195\n",
            "Iteration 409, loss = 0.11879438\n",
            "Iteration 410, loss = 0.11847874\n",
            "Iteration 411, loss = 0.11816467\n",
            "Iteration 412, loss = 0.11785268\n",
            "Iteration 413, loss = 0.11754256\n",
            "Iteration 414, loss = 0.11723439\n",
            "Iteration 415, loss = 0.11692789\n",
            "Iteration 416, loss = 0.11662283\n",
            "Iteration 417, loss = 0.11631961\n",
            "Iteration 418, loss = 0.11601814\n",
            "Iteration 419, loss = 0.11571835\n",
            "Iteration 420, loss = 0.11542038\n",
            "Iteration 421, loss = 0.11512409\n",
            "Iteration 422, loss = 0.11482946\n",
            "Iteration 423, loss = 0.11453653\n",
            "Iteration 424, loss = 0.11424520\n",
            "Iteration 425, loss = 0.11395511\n",
            "Iteration 426, loss = 0.11366674\n",
            "Iteration 427, loss = 0.11337998\n",
            "Iteration 428, loss = 0.11309492\n",
            "Iteration 429, loss = 0.11281150\n",
            "Iteration 430, loss = 0.11252964\n",
            "Iteration 431, loss = 0.11224942\n",
            "Iteration 432, loss = 0.11197149\n",
            "Iteration 433, loss = 0.11169490\n",
            "Iteration 434, loss = 0.11141977\n",
            "Iteration 435, loss = 0.11114559\n",
            "Iteration 436, loss = 0.11087275\n",
            "Iteration 437, loss = 0.11060116\n",
            "Iteration 438, loss = 0.11032941\n",
            "Iteration 439, loss = 0.11005883\n",
            "Iteration 440, loss = 0.10978894\n",
            "Iteration 441, loss = 0.10951999\n",
            "Iteration 442, loss = 0.10925229\n",
            "Iteration 443, loss = 0.10898587\n",
            "Iteration 444, loss = 0.10872055\n",
            "Iteration 445, loss = 0.10845650\n",
            "Iteration 446, loss = 0.10819324\n",
            "Iteration 447, loss = 0.10793062\n",
            "Iteration 448, loss = 0.10766890\n",
            "Iteration 449, loss = 0.10740825\n",
            "Iteration 450, loss = 0.10714908\n",
            "Iteration 451, loss = 0.10689315\n",
            "Iteration 452, loss = 0.10664012\n",
            "Iteration 453, loss = 0.10638944\n",
            "Iteration 454, loss = 0.10614013\n",
            "Iteration 455, loss = 0.10589219\n",
            "Iteration 456, loss = 0.10564564\n",
            "Iteration 457, loss = 0.10540055\n",
            "Iteration 458, loss = 0.10515777\n",
            "Iteration 459, loss = 0.10491694\n",
            "Iteration 460, loss = 0.10467769\n",
            "Iteration 461, loss = 0.10443986\n",
            "Iteration 462, loss = 0.10420342\n",
            "Iteration 463, loss = 0.10396829\n",
            "Iteration 464, loss = 0.10373459\n",
            "Iteration 465, loss = 0.10350220\n",
            "Iteration 466, loss = 0.10327127\n",
            "Iteration 467, loss = 0.10304165\n",
            "Iteration 468, loss = 0.10281335\n",
            "Iteration 469, loss = 0.10258630\n",
            "Iteration 470, loss = 0.10236054\n",
            "Iteration 471, loss = 0.10213645\n",
            "Iteration 472, loss = 0.10191428\n",
            "Iteration 473, loss = 0.10169352\n",
            "Iteration 474, loss = 0.10147403\n",
            "Iteration 475, loss = 0.10125573\n",
            "Iteration 476, loss = 0.10103864\n",
            "Iteration 477, loss = 0.10082286\n",
            "Iteration 478, loss = 0.10060831\n",
            "Iteration 479, loss = 0.10039523\n",
            "Iteration 480, loss = 0.10018342\n",
            "Iteration 481, loss = 0.09997279\n",
            "Iteration 482, loss = 0.09976297\n",
            "Iteration 483, loss = 0.09955427\n",
            "Iteration 484, loss = 0.09934653\n",
            "Iteration 485, loss = 0.09913969\n",
            "Iteration 486, loss = 0.09893382\n",
            "Iteration 487, loss = 0.09872897\n",
            "Iteration 488, loss = 0.09852514\n",
            "Iteration 489, loss = 0.09832238\n",
            "Iteration 490, loss = 0.09812083\n",
            "Iteration 491, loss = 0.09792052\n",
            "Iteration 492, loss = 0.09772137\n",
            "Iteration 493, loss = 0.09752320\n",
            "Iteration 494, loss = 0.09732560\n",
            "Iteration 495, loss = 0.09712864\n",
            "Iteration 496, loss = 0.09693262\n",
            "Iteration 497, loss = 0.09673756\n",
            "Iteration 498, loss = 0.09654414\n",
            "Iteration 499, loss = 0.09635163\n",
            "Iteration 500, loss = 0.09616034\n",
            "–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞:  0.9666666666666667\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ivanu\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ivanu\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.4162338898076595"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits, load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/akmand/datasets/master/iris.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "Y_digits = df['species']\n",
        "X_digits = df.drop(['species'], axis = 1)\n",
        "print(\"Dataset Sizes : \", X_digits.shape, Y_digits.shape)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size = 0.80, test_size = 0.20)\n",
        "print('Train/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes = (100, 100, 100), max_iter = 500, alpha = 0.0001,\n",
        "                    solver = 'sgd', verbose = 10, random_state = 21, tol = 0.000000001)\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred = clf.predict(X_test)\n",
        "print(\"–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: \", accuracy_score(Y_test, Y_pred))\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "print(cm)\n",
        "sns.heatmap(cm, center = True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "X, y = datasets.make_regression(n_samples=200, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
        "regr.predict(X_test[:2])\n",
        "regr.score(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}